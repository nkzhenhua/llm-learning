å¦‚æœä½ çš„æœ€ç»ˆç›®æ ‡æ˜¯å­¦ä¹  LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ï¼Œé‚£ä¹ˆæ¨è ä» PyTorch å¼€å§‹ï¼Œå› ä¸ºç›®å‰å¤§å¤šæ•° LLM ç ”ç©¶å’Œå¼€æºå®ç°ï¼ˆå¦‚ LLaMAã€GPTã€Mistral ç­‰ï¼‰éƒ½æ˜¯åŸºäº PyTorchã€‚

å­¦ä¹ è·¯çº¿æ¨èï¼ˆä»é›¶åˆ° LLMï¼‰

âœ… ç¬¬ä¸€é˜¶æ®µï¼šæŒæ¡æ·±åº¦å­¦ä¹ åŸºç¡€
	â€¢	å…ˆå­¦ PyTorchï¼ˆå®˜æ–¹æ•™ç¨‹ + ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ã€‹ï¼‰
	â€¢	å­¦ä¹ ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†ï¼ˆMLPã€CNNã€RNNï¼‰
	â€¢	ç†Ÿæ‚‰ Transformerï¼ˆæ ¸å¿ƒæ¶æ„ï¼‰

âœ… ç¬¬äºŒé˜¶æ®µï¼šå­¦ä¹  NLP ç›¸å…³æŠ€æœ¯
	â€¢	äº†è§£ tokenizationï¼ˆBPEã€WordPieceï¼‰
	â€¢	ç ”ç©¶ Transformer å’Œ Self-Attention
	â€¢	å®è·µå°å‹ NLP ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ï¼‰

âœ… ç¬¬ä¸‰é˜¶æ®µï¼šæ·±å…¥ LLM è®­ç»ƒä¸å¾®è°ƒ
	â€¢	ç ”ç©¶ GPTã€BERTã€LLaMA çš„æ¶æ„
	â€¢	å­¦ä¹  LoRAã€QLoRAï¼ˆä½æˆæœ¬å¾®è°ƒï¼‰
	â€¢	è¯•ç€ç”¨ Hugging Face Transformers è®­ç»ƒæˆ–å¾®è°ƒ LLM

âœ… ç¬¬å››é˜¶æ®µï¼šä¼˜åŒ–ä¸éƒ¨ç½²
	â€¢	äº†è§£ é‡åŒ–ï¼ˆQuantizationï¼‰ã€è’¸é¦ï¼ˆDistillationï¼‰
	â€¢	è¯•ç€ç”¨ vLLMã€TGIã€TensorRT-LLM éƒ¨ç½²æ¨¡å‹



ğŸ“˜ æ¨èèµ„æº

ğŸ“— åˆçº§ï¼ˆå¿«é€Ÿç†è§£æ¦‚å¿µï¼‰
	â€¢	ã€ŠThe Illustrated Transformerã€‹ by Jay Alammar
ğŸ‘‰ https://jalammar.github.io/illustrated-transformer/
ç”¨å¯è§†åŒ–æ–¹å¼è®² transformerï¼Œéå¸¸é€‚åˆåˆå­¦è€…ã€‚
	â€¢	Hugging Face Courseï¼ˆæ¨èï¼ï¼‰
ğŸ‘‰ https://huggingface.co/learn/nlp-course
	â€¢	Lesson 3: Pretraining
	â€¢	Lesson 4: Fine-tuning
å®Œå…¨é’ˆå¯¹å®é™…æ¨¡å‹è®­ç»ƒè®¾è®¡ï¼Œä½¿ç”¨ transformers åº“ï¼Œå®ç”¨æ€§æé«˜ã€‚
	â€¢	ä¸­æ–‡ï¼šåŠ¨æ‰‹å­¦Transformerï¼ˆç®€ä½“ä¸­æ–‡é¡¹ç›®ï¼‰
ğŸ‘‰ GitHub: https://github.com/datawhalechina/torch-transformers
ä» Transformer æ¶æ„åˆ°é¢„è®­ç»ƒä»»åŠ¡å†åˆ°ä¸‹æ¸¸å¾®è°ƒéƒ½æœ‰æ¶‰åŠã€‚

ğŸ“˜ è¿›é˜¶ï¼ˆæ·±å…¥æœºåˆ¶å’Œå®æˆ˜ï¼‰
	â€¢	è®ºæ–‡æ¨èï¼š
	â€¢	BERT åŸè®ºæ–‡ï¼šBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
	â€¢	GPT åŸè®ºæ–‡ï¼šLanguage Models are Unsupervised Multitask Learners
	â€¢	å®æˆ˜æ¨èï¼ˆLoRA/PEFTï¼‰
	â€¢	PEFT å®˜æ–¹æ–‡æ¡£ï¼šhttps://huggingface.co/docs/peft/index
	â€¢	LoRA æ•™ç¨‹ä¸­æ–‡ç¿»è¯‘ï¼ˆçŸ¥ä¹ï¼‰
ğŸ‘‰ https://zhuanlan.zhihu.com/p/610674306

å­¦ä¹ é¡ºåºå»ºè®®ï¼ˆç»“åˆä¸¤éƒ¨åˆ†ï¼‰
	1.	å…ˆé€šè¿‡ Hugging Face å­¦ä¼š transformer æ¶æ„ã€é¢„è®­ç»ƒä¸å¾®è°ƒæœºåˆ¶
	2.	å†è¯» GPT-3 è®ºæ–‡ï¼Œç†è§£ in-context learning æ˜¯æ€ä¹ˆä¸€å›äº‹
	3.	æ¥ç€ç»ƒä¹  prompt engineering çš„å„ç§æŠ€å·§ï¼ˆfew-shot, CoT, ReActï¼‰
	4.	æœ€åæ·±å…¥ LoRA/QLoRA ç­‰é«˜æ•ˆå¾®è°ƒæ–¹å¼

====================================

éå¸¸å¥½çš„ä¸¤ä¸ªå­¦ä¹ æ–¹å‘ï¼
ä»¥ä¸‹æ˜¯ä½ æåˆ°çš„ä¸¤ä¸ªä¸»é¢˜çš„é«˜è´¨é‡å­¦ä¹ è·¯å¾„å’Œèµ„æºæ¨èï¼ˆè‹±æ–‡èµ„æºä¸ºä¸»ï¼Œä¹Ÿç©¿æ’ä¸€äº›ä¸­æ–‡è®²è§£è§†é¢‘/åšå®¢ï¼‰ï¼š

---

## ğŸ§  1. **é¢„è®­ç»ƒä»»åŠ¡ï¼ˆMasked LMã€Causal LMï¼‰ä¸å¾®è°ƒæŠ€æœ¯**

### âœ… åŸºç¡€æ¦‚å¿µå…¥é—¨

* **Masked LMï¼ˆå¦‚ BERTï¼‰**ï¼šè¾“å…¥ä¸­éšæœº mask æ‰ä¸€äº› tokenï¼Œè®©æ¨¡å‹é¢„æµ‹å‡ºæ¥ã€‚
* **Causal LMï¼ˆå¦‚ GPTï¼‰**ï¼šåªçœ‹å·¦è¾¹ä¸Šä¸‹æ–‡ï¼Œä¸€ä¸ª token æ¥ä¸€ä¸ª token é¢„æµ‹ï¼Œé€‚åˆç”Ÿæˆä»»åŠ¡ã€‚
* **å¾®è°ƒæŠ€æœ¯**ï¼šå¦‚å…¨å‚æ•°å¾®è°ƒï¼ˆFine-tuningï¼‰ã€Adapterã€LoRAã€QLoRAã€PEFT ç­‰ã€‚

### ğŸ“˜ æ¨èèµ„æº

#### ğŸ“— åˆçº§ï¼ˆå¿«é€Ÿç†è§£æ¦‚å¿µï¼‰

* **ã€ŠThe Illustrated Transformerã€‹** by Jay Alammar
  ğŸ‘‰ [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

  > ç”¨å¯è§†åŒ–æ–¹å¼è®² transformerï¼Œéå¸¸é€‚åˆåˆå­¦è€…ã€‚

* **Hugging Face Course**ï¼ˆæ¨èï¼ï¼‰
  ğŸ‘‰ [https://huggingface.co/learn/nlp-course](https://huggingface.co/learn/nlp-course)

  * Lesson 3: Pretraining
  * Lesson 4: Fine-tuning

  > å®Œå…¨é’ˆå¯¹å®é™…æ¨¡å‹è®­ç»ƒè®¾è®¡ï¼Œä½¿ç”¨ `transformers` åº“ï¼Œå®ç”¨æ€§æé«˜ã€‚

* **ä¸­æ–‡ï¼šåŠ¨æ‰‹å­¦Transformer**ï¼ˆç®€ä½“ä¸­æ–‡é¡¹ç›®ï¼‰
  ğŸ‘‰ GitHub: [https://github.com/datawhalechina/torch-transformers](https://github.com/datawhalechina/torch-transformers)

  > ä» Transformer æ¶æ„åˆ°é¢„è®­ç»ƒä»»åŠ¡å†åˆ°ä¸‹æ¸¸å¾®è°ƒéƒ½æœ‰æ¶‰åŠã€‚

#### ğŸ“˜ è¿›é˜¶ï¼ˆæ·±å…¥æœºåˆ¶å’Œå®æˆ˜ï¼‰

* **è®ºæ–‡æ¨è**ï¼š

  * BERT åŸè®ºæ–‡ï¼š[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
  * GPT åŸè®ºæ–‡ï¼š[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

* **å®æˆ˜æ¨èï¼ˆLoRA/PEFTï¼‰**

  * PEFT å®˜æ–¹æ–‡æ¡£ï¼š[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
  * LoRA æ•™ç¨‹ä¸­æ–‡ç¿»è¯‘ï¼ˆçŸ¥ä¹ï¼‰
    ğŸ‘‰ [https://zhuanlan.zhihu.com/p/610674306](https://zhuanlan.zhihu.com/p/610674306)

---

## ğŸ¯ 2. **Prompt Engineering & In-Context Learning åŸç†**

### âœ… æ ¸å¿ƒæ¦‚å¿µ

* **Prompt Engineering**ï¼šç”¨ç‰¹å®šæ¨¡æ¿å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„è¾“å‡ºã€‚
* **In-Context Learningï¼ˆICLï¼‰**ï¼šæ— éœ€ä¿®æ”¹å‚æ•°ï¼Œç»™æ¨¡å‹ä¸€äº›ç¤ºä¾‹ï¼Œå®ƒå°±èƒ½å­¦ä¼šå¦‚ä½•å®Œæˆä»»åŠ¡ã€‚

### ğŸ“˜ æ¨èèµ„æº

#### ğŸ“— åˆçº§ç†è§£

* **Prompt Engineering Guide**ï¼ˆè¶…å…¨ï¼‰
  ğŸ‘‰ [https://github.com/dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)

  > æ¶µç›– Few-shot, Chain-of-Thought (CoT), ReAct ç­‰å¸¸è§ç­–ç•¥ï¼ŒæŒç»­æ›´æ–°ã€‚

* **OpenAI å®˜æ–¹æ•™ç¨‹ï¼šPrompt Engineering for Developers**
  ğŸ‘‰ [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)

  > ä¸“ä¸º ChatGPT/GPT-4 ä½¿ç”¨è€…è®¾è®¡ã€‚

* **ä¸­æ–‡è®²è§£è§†é¢‘ï¼ˆæ¨è Bç«™ï¼‰**

  * æœç´¢ â€œPrompt Engineering åŸç†â€â€œIn-Context Learning æœºåˆ¶â€ç­‰å…³é”®è¯ï¼Œæœ‰ä¸å°‘ NLP åšä¸»è®²è§£å¾—ä¸é”™ã€‚
  * æ¨èï¼šé£æ¡¨ã€Hugging Face ä¸­æ–‡ç¤¾åŒºç›¸å…³è§†é¢‘ã€‚

#### ğŸ“˜ è¿›é˜¶èµ„æº

* **è®ºæ–‡æ¨è**ï¼š

  * GPT-3 åŸè®ºæ–‡ï¼ˆICL ç†è®ºæ¥æºï¼‰
    ğŸ‘‰ [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
  * Chain-of-Thought Prompting paper
    ğŸ‘‰ [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)

    > å¤§å¤§æå‡äº†å¤æ‚æ¨ç†ç±»é—®é¢˜çš„è¡¨ç°

* **è§†é¢‘è¯¾ï¼šCS324 @ Stanford (Large Language Models)**
  ğŸ‘‰ [https://web.stanford.edu/class/cs324/](https://web.stanford.edu/class/cs324/)

  > æœ€æ–°å‰æ²¿å†…å®¹ï¼ŒåŒ…å« ICLã€promptã€alignment ç­‰ LLM æ ¸å¿ƒä¸»é¢˜

---

## ğŸ§© å­¦ä¹ é¡ºåºå»ºè®®ï¼ˆç»“åˆä¸¤éƒ¨åˆ†ï¼‰

1. å…ˆé€šè¿‡ Hugging Face å­¦ä¼š transformer æ¶æ„ã€é¢„è®­ç»ƒä¸å¾®è°ƒæœºåˆ¶
2. å†è¯» GPT-3 è®ºæ–‡ï¼Œç†è§£ in-context learning æ˜¯æ€ä¹ˆä¸€å›äº‹
3. æ¥ç€ç»ƒä¹  prompt engineering çš„å„ç§æŠ€å·§ï¼ˆfew-shot, CoT, ReActï¼‰
4. æœ€åæ·±å…¥ LoRA/QLoRA ç­‰é«˜æ•ˆå¾®è°ƒæ–¹å¼

---

å¤ªå¥½äº†ï¼Œä¸‹é¢æ˜¯ä¸ºä½ é‡èº«å®šåˆ¶çš„ **8 å‘¨ LLM å­¦ä¹ è®¡åˆ’è¡¨**ï¼Œç›®æ ‡æ˜¯ï¼šæŒæ¡ LLM çš„æ ¸å¿ƒåŸç†ã€å¾®è°ƒæ–¹æ³•ã€Prompt å·¥ç¨‹ã€æ¨ç†æœºåˆ¶ï¼Œæœ€ç»ˆèƒ½å¤Ÿç‹¬ç«‹æ„å»ºå’Œéƒ¨ç½²ä¸€ä¸ª LLM åº”ç”¨ã€‚

---

# ğŸ§­ 8 å‘¨ LLM å­¦ä¹ è®¡åˆ’è¡¨ï¼ˆå·¥ç¨‹+ç ”ç©¶æ–¹å‘ï¼‰

### ğŸ‘¨â€ğŸ’» æ¯å‘¨å¤§çº¦æŠ•å…¥ï¼š8ï½12 å°æ—¶ï¼ˆå¯çµæ´»è°ƒæ•´ï¼‰

æ¯å‘¨åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š

* ğŸ§  ç†è®ºå­¦ä¹ 
* ğŸ› ï¸ å®è·µæ“ä½œ
* ğŸ”— èµ„æ–™é“¾æ¥

---

## **Week 1ï¼šTransformer æ¶æ„ä¸é¢„è®­ç»ƒæ¦‚å¿µ**

### ğŸ§  ç†è®º

* Transformer æ¶æ„ä¸ Attention
* Masked LM vs Causal LM
* é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡åŒºåˆ«

### ğŸ› ï¸ å®è·µ

* ç”¨ `transformers` åŠ è½½ BERT å’Œ GPT-2
* è®© GPT-2 ç”Ÿæˆæ–‡æœ¬ï¼Œä½“éªŒ causal LM

### ğŸ”— èµ„æºé“¾æ¥

* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
* [Hugging Face NLP Course Chapter 1â€“4](https://huggingface.co/learn/nlp-course)
* [Transformers æ–‡æ¡£å…¥é—¨](https://huggingface.co/docs/transformers/index)

---

## **Week 2ï¼šHugging Face å®æˆ˜ + è‡ªå®šä¹‰æ•°æ®å¾®è°ƒ BERT**

### ğŸ§  ç†è®º

* Tokenizer å·¥ä½œåŸç†
* Fine-tuning åŸºç¡€æµç¨‹

### ğŸ› ï¸ å®è·µ

* å¾®è°ƒ BERT åšæƒ…æ„Ÿåˆ†ææˆ–æ–‡æœ¬åˆ†ç±»ï¼ˆIMDb æ•°æ®é›†ï¼‰
* ä½¿ç”¨ Datasets + Trainer API

### ğŸ”— èµ„æºé“¾æ¥

* [Hugging Face æ–‡æ¡£ - Fine-tuning BERT](https://huggingface.co/docs/transformers/training)
* [IMDb æ•°æ®é›†](https://huggingface.co/datasets/imdb)
* [å®˜æ–¹å¾®è°ƒæ•™ç¨‹ notebook](https://github.com/huggingface/notebooks/blob/main/course/en/chapter3/section3.ipynb)

---

## **Week 3ï¼šCausal LM è®­ç»ƒ + GPT-2 æ–‡æœ¬ç”ŸæˆæŠ€å·§**

### ğŸ§  ç†è®º

* è§£ç ç­–ç•¥ï¼ˆGreedyã€Beamã€Top-kã€Top-pï¼‰
* æ¸©åº¦ä¸é‡å¤æƒ©ç½šçš„å«ä¹‰

### ğŸ› ï¸ å®è·µ

* ç”¨ GPT-2 åšæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å†™è¯—ã€å¯¹è¯ï¼‰
* å¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥æ•ˆæœ

### ğŸ”— èµ„æºé“¾æ¥

* [æ–‡æœ¬ç”Ÿæˆè¯¦è§£åšå®¢](https://huggingface.co/blog/how-to-generate)
* [GPT2 ç”Ÿæˆæ–‡æœ¬ notebook ç¤ºä¾‹](https://github.com/huggingface/notebooks/blob/main/examples/text_generation.ipynb)

---

## **Week 4ï¼šPrompt Engineering & In-Context Learning**

### ğŸ§  ç†è®º

* Zero-shot / Few-shot Prompt
* Chain-of-Thoughtï¼ˆCoTï¼‰Prompting
* ReAct æ¨¡å‹ç®€ä»‹

### ğŸ› ï¸ å®è·µ

* æ„é€  prompt æ¥å®ç°æƒ…æ„Ÿåˆ†ç±»ã€é€»è¾‘é¢˜è§£ç­”
* å¯¹æ¯”æœ‰æ—  CoT æ—¶çš„è¡¨ç°å·®å¼‚

### ğŸ”— èµ„æºé“¾æ¥

* [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
* [CoT Prompting è®ºæ–‡](https://arxiv.org/abs/2201.11903)
* [OpenAI Prompt Guide](https://platform.openai.com/docs/guides/prompt-engineering)

---

## **Week 5ï¼šLoRA / PEFT å¾®è°ƒæŠ€æœ¯**

### ğŸ§  ç†è®º

* å…¨å‚æ•°å¾®è°ƒ vs LoRA/Adapter
* PEFT æ ¸å¿ƒæ€æƒ³ï¼šå†»ç»“å¤§æ¨¡å‹ï¼Œä»…å¾®è°ƒå°‘æ•°å±‚

### ğŸ› ï¸ å®è·µ

* ä½¿ç”¨ `peft` å¾®è°ƒ LLaMA/Mistral/ChatGLM æ¨¡å‹
* å°è¯•ç”¨ QLoRA åœ¨ä½æ˜¾å­˜ç¯å¢ƒè®­ç»ƒæ¨¡å‹

### ğŸ”— èµ„æºé“¾æ¥

* [PEFT å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/peft)
* [QLoRA åŸè®ºæ–‡](https://arxiv.org/abs/2305.14314)
* [ä¸­æ–‡ QLoRA æ•™ç¨‹](https://zhuanlan.zhihu.com/p/640898922)

---

## **Week 6ï¼šLLM åº”ç”¨å¼€å‘ + Gradio å‰ç«¯**

### ğŸ§  ç†è®º

* å¦‚ä½•éƒ¨ç½²å¾®è°ƒæ¨¡å‹
* Gradio/Streamlit å‰ç«¯æ¡†æ¶

### ğŸ› ï¸ å®è·µ

* æ„å»ºä¸€ä¸ª Chatbot ç•Œé¢
* æ”¯æŒ prompt è¾“å…¥ã€è®¾ç½®æ¸©åº¦ã€max tokens ç­‰å‚æ•°

### ğŸ”— èµ„æºé“¾æ¥

* [Gradio å…¥é—¨æ–‡æ¡£](https://www.gradio.app/)
* [éƒ¨ç½² Hugging Face æ¨¡å‹ + Gradio æ•™ç¨‹](https://huggingface.co/blog/gradio)

---

## **Week 7ï¼šRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿå¼€å‘**

### ğŸ§  ç†è®º

* RAG åŸç†ï¼šæœç´¢ + ç”Ÿæˆ
* å‘é‡æ£€ç´¢åº“ï¼ˆFAISS / Chroma / LlamaIndexï¼‰

### ğŸ› ï¸ å®è·µ

* ç”¨ FAISS å»ºç«‹æœ¬åœ°æ–‡æ¡£æœç´¢
* ç”¨ LangChain æˆ– LlamaIndex æ„å»º RAG pipeline

### ğŸ”— èµ„æºé“¾æ¥

* [Hugging Face RAG æ•™ç¨‹](https://huggingface.co/docs/transformers/main/en/tasks/retrieval)
* [LangChain ä¸­æ–‡æ•™ç¨‹](https://zhuanlan.zhihu.com/p/636471142)
* [LlamaIndex å…¥é—¨](https://docs.llamaindex.ai/en/stable/)

---

## **Week 8ï¼šå¯¹é½æ–¹æ³•ï¼ˆAlignmentï¼‰ä¸æ¨¡å‹èƒ½åŠ›æå‡**

### ğŸ§  ç†è®º

* RLHF vs DPO
* Preference æ•°æ®æ”¶é›†ä¸å»ºæ¨¡
* æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰

### ğŸ› ï¸ å®è·µ

* è¯•éªŒ `DPOTrainer` è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼ˆä½¿ç”¨ `trl`ï¼‰
* é˜…è¯»å¹¶å¤ç° Alpaca/LIMA é¡¹ç›®å¾®è°ƒ

### ğŸ”— èµ„æºé“¾æ¥

* [DPO åŸè®ºæ–‡](https://arxiv.org/abs/2305.18290)
* [trl: Transformer Reinforcement Learning](https://huggingface.co/docs/trl/index)
* [Alpaca å¤ç°é¡¹ç›®](https://github.com/tatsu-lab/stanford_alpaca)

---

## âœ… å­¦å®Œåä½ å°†èƒ½ï¼š

* ç†è§£ LLM çš„æ ¸å¿ƒæœºåˆ¶ä¸è®­ç»ƒæ–¹æ³•
* ç†Ÿç»ƒä½¿ç”¨ Hugging Face è®­ç»ƒä¸éƒ¨ç½²æ¨¡å‹
* æ„å»ºå¾®è°ƒ/å¯¹è¯/RAG ç­‰å®é™… LLM åº”ç”¨
* é˜…è¯» LLM é¢†åŸŸçš„ä¸»æµè®ºæ–‡ï¼Œå¹¶å¿«é€Ÿå¤ç°æ ¸å¿ƒæŠ€æœ¯

---

